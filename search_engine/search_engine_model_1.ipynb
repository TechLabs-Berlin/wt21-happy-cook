{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import advertools as adv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_interaction = pd.read_csv('ds_dataset/raw-data_interaction.csv')\n",
    "rd_recipe = pd.read_csv('ds_dataset/raw-data_recipe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data in 'cooking directions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    \n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "\n",
    "        for character in w:\n",
    "            if character.isdigit():\n",
    "                w = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", w)\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "        \n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n",
    "    \n",
    "    data = np.char.replace(data, \" m\\\\n\", ' minutes ' )\n",
    "    data = np.char.replace(data, 'h\\\\', ' hours ')\n",
    "    data = np.char.replace(data, ' h ', ' hours ')\n",
    "\n",
    "    data = np.char.replace(data, \"\\\\n\", ' ')\n",
    "\n",
    "    for i in range(len(symbols)):\n",
    "        \n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "        \n",
    "    data = np.char.replace(data, ',', '')\n",
    "    data = np.char.replace(data, ' f ', ' fahrenheit ')\n",
    "    data = np.char.replace(data, ' c ', ' celcius ')\n",
    "    data = np.char.replace(data, \" u'\", ' ' )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_otherwords(data):\n",
    "    data = np.char.replace(data, 'prep', ' ')\n",
    "    data = np.char.replace(data, 'directions', ' ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "for text in rd_recipe['cooking_directions']:\n",
    "    processed_text.append(word_tokenize(preprocess(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating DF of all words\n",
    "\n",
    "DF is the count of occurences of term t in the document set N. \n",
    "In other words, DF is the no. of documents in which the word is present\n",
    "\n",
    "df(t) = occurence of t in N documents\n",
    "\n",
    "To keep this also in a range, we normalize by dividing by the total no. of documents. Our main goal is to know the INFORMATIVENESS of a term. The higher the no. of DF, the less informativeness the term has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DF = {}\n",
    "\n",
    "N = len(processed_text)\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    for w in tokens:\n",
    "        #print(w)\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "\n",
    "\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see below that \"directions\" pops up in every recipe. so it is the least informative term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directions', 49698), ('prep', 44217), ('five', 36921), ('ncook', 36457)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DF.items())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9079"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_size = len(DF)\n",
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['directions',\n",
       " 'prep',\n",
       " 'five',\n",
       " 'ncook',\n",
       " 'two',\n",
       " 'hours',\n",
       " 'forty',\n",
       " 'minutes',\n",
       " 'ready',\n",
       " 'eleven']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = [x for x in DF]\n",
    "total_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF is individual to each document and word. \n",
    "\n",
    "IDF is the inverse of the document frequency which is proportional to the informativeness of term t. When we calculate IDF, it will be very low for the most common words such as stop words. N/df therefore would be low. This gives what we want, a relative weightage.\n",
    "\n",
    "If idf(t) = N/df, we get singularity when N is too big and df is too small. So the smoothest formula is shown below.\n",
    "\n",
    "tf(t,d) = count of t in d / number of words in d\n",
    "\n",
    "df(t) = occurence of t in N documents\n",
    "\n",
    "idf(t) = log(N/(df+1))\n",
    "\n",
    "Finally, by taking a multiplicative value of TF and IDF, we get TF-IDF score. There are many different variations of TF-IDF but for now let us concentrate on this basic version:\n",
    "\n",
    "tf-idf(t,d) = tf(t,d) * log(N/(df+1))\n",
    "\n",
    "We use tf-idf values to represent the weight of each term within each document. A series of tf-idf values of our total_vocab will form the vector of our document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to calculate similarities between queries/documents. In this method, we need to convert our text data into numerical values. Here, using gen_vector(), we have converted a series of words/strings into a vector. The vector is composed of tf-idf values of a token. Then, using cosine_sim, we calculate the similarity between our query vector and each vector of our 'cooking_directions' doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "\n",
    "    print(query_vector)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "\n",
      "Query: pork skillet soup potatoes bake fry pepper\n",
      "\n",
      "['pork', 'skillet', 'soup', 'potatoes', 'bake', 'fry', 'pepper']\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[  403 13952  7229 13059  9539  7500 13392  4099   573  9255]\n"
     ]
    }
   ],
   "source": [
    "Q = cosine_similarity(10, \"pork skillet soup potatoes bake fry pepper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " directions prep five ncook twenty five minutes ready thirty minutes season pork salt black pepper heat one tablespoon olive oil twelve inch skillet medium high heat add pork batches cook six minutes browned sides remove pork skillet add onion skillet cook five minutes tender crisp stirring occasionally stir honey mustard soup heat boil return pork skillet reduce heat low cook five minutes pork cooked serve pork soup mixture potatoes\n",
      " \n",
      " boneless pork tenderloins medium onion honey dijon style mustard carton campbellsÂ® sweet onion soup hot mashed potatoes\n",
      " \n",
      " sweet onion pork medallions\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(rd_recipe['cooking_directions'][403]))\n",
    "print(\" \")\n",
    "print(preprocess(rd_recipe['ingredients'][403]))\n",
    "print(\" \")\n",
    "print(preprocess(rd_recipe['recipe_name'][403]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
