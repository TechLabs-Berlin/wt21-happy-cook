{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'advertools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-619-7da8d74575e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0madvertools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'advertools'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "from scipy.special import logsumexp\n",
    "import advertools as adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_interaction = pd.read_csv('raw-data_interaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rd_recipe = pd.read_csv('core-data_recipe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data in 'cooking directions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    \n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "\n",
    "        for character in w:\n",
    "            if character.isdigit():\n",
    "                w = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", w)\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "        \n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n",
    "    \n",
    "    data = np.char.replace(data, \" m\\\\n\", ' minutes ' )\n",
    "    data = np.char.replace(data, 'h\\\\', ' hours ')\n",
    "    data = np.char.replace(data, ' h ', ' hours ')\n",
    "\n",
    "    data = np.char.replace(data, \"\\\\n\", ' ')\n",
    "\n",
    "    for i in range(len(symbols)):\n",
    "        \n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "        \n",
    "    data = np.char.replace(data, ',', '')\n",
    "    data = np.char.replace(data, ' f ', ' fahrenheit ')\n",
    "    data = np.char.replace(data, ' c ', ' celcius ')\n",
    "    data = np.char.replace(data, \" u'\", ' ' )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_otherwords(data):\n",
    "    data = np.char.replace(data, 'prep', ' ')\n",
    "    data = np.char.replace(data, 'directions', ' ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_stop_words(data)#needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "for text in rd_recipe['cooking_directions']:\n",
    "    processed_text.append(word_tokenize(preprocess(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating DF of all words\n",
    "\n",
    "DF is the count of occurences of term t in the document set N. \n",
    "In other words, DF is the no. of documents in which the word is present\n",
    "\n",
    "df(t) = occurence of t in N documents\n",
    "\n",
    "To keep this also in a range, we normalize by dividing by the total no. of documents. Our main goal is to know the INFORMATIVENESS of a term. The higher the no. of DF, the less informativeness the term has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DF = {}\n",
    "\n",
    "N = len(processed_text)\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    for w in tokens:\n",
    "        #print(w)\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "\n",
    "\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see below that \"directions\" pops up in every recipe. so it is the least informative term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directions', 45630), ('prep', 40489), ('fifteen', 20274), ('ncook', 33511)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DF.items())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8629"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_size = len(DF)\n",
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['directions',\n",
       " 'prep',\n",
       " 'fifteen',\n",
       " 'ncook',\n",
       " 'two',\n",
       " 'hours',\n",
       " 'thirty',\n",
       " 'minutes',\n",
       " 'ready',\n",
       " 'forty']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = [x for x in DF]\n",
    "total_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF is individual to each document and word. \n",
    "\n",
    "IDF is the inverse of the document frequency which is proportional to the informativeness of term t. When we calculate IDF, it will be very low for the most common words such as stop words. N/df therefore would be low. This gives what we want, a relative weightage.\n",
    "\n",
    "If idf(t) = N/df, we get singularity when N is too big and df is too small. So the smoothest formula is shown below.\n",
    "\n",
    "tf(t,d) = count of t in d / number of words in d\n",
    "\n",
    "df(t) = occurence of t in N documents\n",
    "\n",
    "idf(t) = log(N/(df+1))\n",
    "\n",
    "Finally, by taking a multiplicative value of TF and IDF, we get TF-IDF score. There are many different variations of TF-IDF but for now let us concentrate on this basic version:\n",
    "\n",
    "tf-idf(t,d) = tf(t,d) * log(N/(df+1))\n",
    "\n",
    "We use tf-idf values to represent the weight of each term within each document. A series of tf-idf values of our total_vocab will form the vector of our document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to calculate similarities between queries/documents. In this method, we need to convert our text data into numerical values. Here, using gen_vector(), we have converted a series of words/strings into a vector. The vector is composed of tf-idf values of a token. Then, using cosine_sim, we calculate the similarity between our query vector and each vector of our 'cooking_directions' doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "\n",
    "    print(query_vector)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken potato onion\n",
      "Cosine Similarity\n",
      "\n",
      "Query: chicken potato onion\n",
      "\n",
      "['chicken', 'potato', 'onion']\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[ 5199 26314  4464 36696 27133 16734 26025  8564 45164 27376]\n"
     ]
    }
   ],
   "source": [
    "query=input(str())\n",
    "Q=cosine_similarity(10,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluation(Q):\n",
    "    DQ=pd.DataFrame(Q,columns=['id'])\n",
    "    token_query=word_tokenize(str(preprocess(query)))\n",
    "    QL=len(token_query)\n",
    "    DQ['ingredient']=DQ.id.apply(lambda x: rd_recipe['ingredients'][x]) #get the ingredient of the recipes\n",
    "    DQ['ingredient']=DQ.ingredient.apply(lambda x: word_tokenize(str(preprocess(x)))) #get the ingredient processed\n",
    "    DQ['fit']=DQ.ingredient.apply(lambda x: [n for n in x if n in token_query]) #get the ingredients that match the request\n",
    "    DQ['fit']=DQ['fit'].apply(np.unique)\n",
    "    DQ['number'] = DQ['fit'].str.len()# get the number of the ingredients that match the query\n",
    "    DQ['percent']=DQ['number']/QL # get the propotion that fits\n",
    "    DQ.drop(columns=['fit','number'], inplace=True)\n",
    "    return DQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredient</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5199</td>\n",
       "      <td>[larg, russet, potato, butter, skinless, bonel...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26314</td>\n",
       "      <td>[medium, sweet, potato, margarin, soften, crus...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4464</td>\n",
       "      <td>[curri, powder, season, salt, onion, powder, l...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36696</td>\n",
       "      <td>[larg, russet, potato, veget, oil, butter, min...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27133</td>\n",
       "      <td>[russet, potato, red, potato, sweet, potato, o...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16734</td>\n",
       "      <td>[larg, russet, potato, oliv, oil, salt, ground...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26025</td>\n",
       "      <td>[medium, bake, potato, medium, onion, butter, ...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8564</td>\n",
       "      <td>[oliv, oil, larg, skinless, boneless, chicken,...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45164</td>\n",
       "      <td>[sweet, potato, wheat, flour, unsweeten, apple...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27376</td>\n",
       "      <td>[taco, fill, oil, carrot, potato, leek, chop, ...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                         ingredient   percent\n",
       "0   5199  [larg, russet, potato, butter, skinless, bonel...  1.000000\n",
       "1  26314  [medium, sweet, potato, margarin, soften, crus...  0.333333\n",
       "2   4464  [curri, powder, season, salt, onion, powder, l...  1.000000\n",
       "3  36696  [larg, russet, potato, veget, oil, butter, min...  0.666667\n",
       "4  27133  [russet, potato, red, potato, sweet, potato, o...  0.666667\n",
       "5  16734  [larg, russet, potato, oliv, oil, salt, ground...  0.333333\n",
       "6  26025  [medium, bake, potato, medium, onion, butter, ...  0.666667\n",
       "7   8564  [oliv, oil, larg, skinless, boneless, chicken,...  1.000000\n",
       "8  45164  [sweet, potato, wheat, flour, unsweeten, apple...  0.333333\n",
       "9  27376  [taco, fill, oil, carrot, potato, leek, chop, ...  0.666667"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
