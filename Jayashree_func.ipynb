{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d46f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jayshree_func(filename,column1,column2):\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "#import advertools as adv\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from collections import Counter\n",
    "#from num2words import num2words\n",
    "    import nltk\n",
    "    import os\n",
    "    import string\n",
    "    import numpy as np\n",
    "    import copy\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import re\n",
    "    import math\n",
    "    from scipy.special import logsumexp\n",
    "    RD = pd.read_csv('filename')\n",
    "    RD[\"Total\"] =RD[\"column1\"] + RD[\"column2\"]\n",
    "    def convert_lower_case(data):\n",
    "        return np.char.lower(data)\n",
    "    def remove_stop_words(data):\n",
    "        stop_words = stopwords.words('english')\n",
    "        words = word_tokenize(str(data))\n",
    "        new_text = \"\"\n",
    "        for w in words:\n",
    "            if w not in stop_words and len(w) > 1:\n",
    "                new_text = new_text + \" \" + w\n",
    "        return new_text\n",
    "    def remove_apostrophe(data):\n",
    "        return np.char.replace(data, \"'\", \"\")\n",
    "    def stemming(data):\n",
    "        stemmer= PorterStemmer()\n",
    "    \n",
    "        tokens = word_tokenize(str(data))\n",
    "        new_text = \"\"\n",
    "        for w in tokens:\n",
    "            new_text = new_text + \" \" + stemmer.stem(w)\n",
    "        return new_text\n",
    "    def convert_numbers(data):\n",
    "        tokens = word_tokenize(str(data))\n",
    "    \n",
    "        new_text = \"\"\n",
    "        for w in tokens:\n",
    "\n",
    "            for character in w:\n",
    "                if character.isdigit():\n",
    "                    w = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", w)\n",
    "        \n",
    "            try:\n",
    "            \n",
    "                w = num2words(int(w))\n",
    "            except:\n",
    "                a = 0\n",
    "            new_text = new_text + \" \" + w\n",
    "        \n",
    "        new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    \n",
    "        return new_text\n",
    "    def remove_punctuation(data):\n",
    "        symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n",
    "    \n",
    "        data = np.char.replace(data, \" m\\\\n\", ' minutes ' )\n",
    "        data = np.char.replace(data, 'h\\\\', ' hours ')\n",
    "        data = np.char.replace(data, ' h ', ' hours ')\n",
    "\n",
    "        data = np.char.replace(data, \"\\\\n\", ' ')\n",
    "\n",
    "        for i in range(len(symbols)):\n",
    "        \n",
    "            data = np.char.replace(data, symbols[i], ' ')\n",
    "            data = np.char.replace(data, \"  \", \" \")\n",
    "        \n",
    "        data = np.char.replace(data, ',', '')\n",
    "        data = np.char.replace(data, ' f ', ' fahrenheit ')\n",
    "        data = np.char.replace(data, ' c ', ' celcius ')\n",
    "        data = np.char.replace(data, \" u'\", ' ' )\n",
    "\n",
    "        return data\n",
    "    def remove_otherwords(data):\n",
    "        data = np.char.replace(data, 'prep', ' ')\n",
    "        data = np.char.replace(data, 'directions', ' ')\n",
    "        return data\n",
    "    def preprocess(data):\n",
    "        data = convert_lower_case(data)\n",
    "        data = convert_numbers(data)\n",
    "        data = remove_punctuation(data) \n",
    "        data = convert_numbers(data)\n",
    "        data = remove_apostrophe(data)\n",
    "        data = remove_stop_words(data)\n",
    "        data = convert_numbers(data)\n",
    "        data = remove_punctuation(data)\n",
    "        data = convert_numbers(data)\n",
    "        data = remove_stop_words(data) \n",
    "        return data\n",
    "    processed_text = []\n",
    "    for text in RD['cooking_directions']:\n",
    "        processed_text.append(word_tokenize(preprocess(text)))\n",
    "        print(preprocess(RD['cooking_directions'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5e297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
